{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Imports that enable human-in-the-loop implementation\n",
    "from langgraph.types import Command, interrupt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "# Import the AgentCoreMemorySaver that we will use as a checkpointer\n",
    "from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.getLogger(\"support-agent\").setLevel(logging.INFO)\n",
    "region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "\n",
    "logger = logging.getLogger(\"support-agent\")"
   ],
   "id": "ad87856643f5e8b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "memory_name = \"SupportAgent\"\n",
    "\n",
    "client = MemoryClient(region_name=region)\n",
    "memory = client.create_or_get_memory(name=memory_name)\n",
    "memory_id = memory[\"id\"]"
   ],
   "id": "6e8aed657f21e2a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Initialize checkpointer for state persistence\n",
    "checkpointer = AgentCoreMemorySaver(memory_id, region_name=region)"
   ],
   "id": "9a3507f5586069ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"\"\"Add two integers and return the result\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int):\n",
    "    \"\"\"Multiply two integers and return the result\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply, human_assistance]"
   ],
   "id": "d27e80358b0390f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)\n",
    "\n",
    "graph = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "graph"
   ],
   "id": "ff9823c93c18d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_input = \"I would like to work with a customer service human agent.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"actor_id\": \"demo-notebook\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "69e994cc99164d81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ],
   "id": "363f04bd007fdd85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "human_response = (\n",
    "    \"I'm sorry to hear that you are frustrated. Looking at the past conversation history, I can see that you've requested a refund. I've gone ahead and credited it to your account.\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"messages\": human_response})\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ],
   "id": "b7626884089298a6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
